{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AluminatAI GPU Agent - A100 Test Suite\n",
        "\n",
        "**Requirements:** Google Colab with A100 GPU runtime\n",
        "\n",
        "This notebook tests:\n",
        "1. GPU collector on real A100 hardware\n",
        "2. Metrics data integrity and validation\n",
        "3. API key authentication and ingest endpoint\n",
        "4. End-to-end flow: collect → upload → verify\n",
        "5. Energy calculation accuracy\n",
        "6. Stress test under GPU load\n",
        "\n",
        "---\n",
        "\n",
        "## Setup\n",
        "\n",
        "Go to **Runtime > Change runtime type** and select **A100 GPU** before running."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Verify GPU & Install Dependencies\n",
        "!nvidia-smi\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "import subprocess\n",
        "result = subprocess.run(['nvidia-smi', '--query-gpu=name,uuid,memory.total,power.limit',\n",
        "                         '--format=csv,noheader'], capture_output=True, text=True)\n",
        "print(f\"GPU Info: {result.stdout.strip()}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!pip install -q nvidia-ml-py3 requests rich\n",
        "print(\"\\nDependencies installed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Configuration\n",
        "# ==========================================\n",
        "# PASTE YOUR API KEY HERE (from dashboard)\n",
        "# ==========================================\n",
        "API_KEY = \"\"  # e.g. \"alum_AbCdEf12345...\"\n",
        "API_ENDPOINT = \"https://aluminatiai-landing.vercel.app/api/metrics/ingest\"\n",
        "\n",
        "# Test settings\n",
        "SAMPLE_INTERVAL = 2.0  # seconds between samples\n",
        "NUM_SAMPLES = 10       # number of samples to collect\n",
        "\n",
        "print(f\"API Endpoint: {API_ENDPOINT}\")\n",
        "print(f\"API Key: {'alum_****' + API_KEY[-4:] if API_KEY else 'NOT SET'}\")\n",
        "print(f\"Samples: {NUM_SAMPLES} at {SAMPLE_INTERVAL}s intervals\")\n",
        "\n",
        "if not API_KEY:\n",
        "    print(\"\\n⚠️  WARNING: No API key set. Upload tests will be skipped.\")\n",
        "    print(\"   Sign up at https://aluminatiai-landing.vercel.app to get your key.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 1: GPU Collector (NVML)\n",
        "\n",
        "Test that we can initialize the NVML collector and read GPU metrics from the A100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Test NVML Collector\n",
        "import pynvml\n",
        "import time\n",
        "from datetime import datetime, timezone\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "passed = 0\n",
        "failed = 0\n",
        "\n",
        "# --- Test 1.1: NVML Initialization ---\n",
        "try:\n",
        "    pynvml.nvmlInit()\n",
        "    gpu_count = pynvml.nvmlDeviceGetCount()\n",
        "    assert gpu_count >= 1, \"Expected at least 1 GPU\"\n",
        "    print(f\"✅ Test 1.1: NVML initialized - {gpu_count} GPU(s) found\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 1.1: NVML init failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 1.2: GPU Info Retrieval ---\n",
        "try:\n",
        "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "    gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
        "    gpu_uuid = pynvml.nvmlDeviceGetUUID(handle)\n",
        "    if isinstance(gpu_name, bytes): gpu_name = gpu_name.decode('utf-8')\n",
        "    if isinstance(gpu_uuid, bytes): gpu_uuid = gpu_uuid.decode('utf-8')\n",
        "\n",
        "    assert 'A100' in gpu_name or 'GPU' in gpu_name, f\"Unexpected GPU: {gpu_name}\"\n",
        "    assert len(gpu_uuid) > 10, \"UUID too short\"\n",
        "    print(f\"✅ Test 1.2: GPU info - {gpu_name} ({gpu_uuid[:20]}...)\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 1.2: GPU info failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 1.3: Power Reading ---\n",
        "try:\n",
        "    power_mw = pynvml.nvmlDeviceGetPowerUsage(handle)\n",
        "    power_w = power_mw / 1000.0\n",
        "    power_limit = pynvml.nvmlDeviceGetPowerManagementLimit(handle) / 1000.0\n",
        "\n",
        "    assert 0 <= power_w <= 1500, f\"Power out of range: {power_w}W\"\n",
        "    assert power_limit > 0, f\"Power limit invalid: {power_limit}W\"\n",
        "    print(f\"✅ Test 1.3: Power reading - {power_w:.1f}W / {power_limit:.0f}W limit\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 1.3: Power reading failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 1.4: Utilization ---\n",
        "try:\n",
        "    util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
        "    assert 0 <= util.gpu <= 100, f\"GPU util out of range: {util.gpu}%\"\n",
        "    assert 0 <= util.memory <= 100, f\"Mem util out of range: {util.memory}%\"\n",
        "    print(f\"✅ Test 1.4: Utilization - GPU: {util.gpu}%, Memory: {util.memory}%\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 1.4: Utilization failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 1.5: Temperature ---\n",
        "try:\n",
        "    temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
        "    assert 0 <= temp <= 120, f\"Temperature out of range: {temp}C\"\n",
        "    print(f\"✅ Test 1.5: Temperature - {temp}°C\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 1.5: Temperature failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 1.6: Memory ---\n",
        "try:\n",
        "    mem = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "    mem_used_mb = mem.used / 1024 / 1024\n",
        "    mem_total_mb = mem.total / 1024 / 1024\n",
        "    assert mem_total_mb > 1000, f\"Total memory too low: {mem_total_mb}MB\"\n",
        "    assert mem_used_mb >= 0, f\"Used memory negative: {mem_used_mb}MB\"\n",
        "    print(f\"✅ Test 1.6: Memory - {mem_used_mb:.0f}MB / {mem_total_mb:.0f}MB\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 1.6: Memory failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "pynvml.nvmlShutdown()\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Test 1 Results: {passed} passed, {failed} failed\")\n",
        "print(f\"{'='*50}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 2: Full Collector Class\n",
        "\n",
        "Test the GPUCollector class with energy delta calculations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Define GPUCollector inline (same as agent/collector.py)\n",
        "import pynvml\n",
        "import time\n",
        "from datetime import datetime, timezone\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPUMetrics:\n",
        "    \"\"\"Single GPU metrics snapshot\"\"\"\n",
        "    timestamp: str\n",
        "    gpu_index: int\n",
        "    gpu_uuid: str\n",
        "    gpu_name: str\n",
        "    power_draw_w: float\n",
        "    power_limit_w: float\n",
        "    energy_delta_j: Optional[float] = None\n",
        "    utilization_gpu_pct: int = 0\n",
        "    utilization_memory_pct: int = 0\n",
        "    temperature_c: int = 0\n",
        "    fan_speed_pct: int = 0\n",
        "    sm_clock_mhz: Optional[int] = None\n",
        "    memory_clock_mhz: Optional[int] = None\n",
        "    memory_used_mb: float = 0\n",
        "    memory_total_mb: float = 0\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return asdict(self)\n",
        "\n",
        "\n",
        "class GPUCollector:\n",
        "    def __init__(self, collect_clocks: bool = False):\n",
        "        self.collect_clocks = collect_clocks\n",
        "        self.initialized = False\n",
        "        self.gpu_count = 0\n",
        "        self.gpu_handles = []\n",
        "        self.gpu_info = []\n",
        "        self.last_sample_time = {}\n",
        "        self.last_power_draw = {}\n",
        "        self._initialize()\n",
        "\n",
        "    def _initialize(self):\n",
        "        pynvml.nvmlInit()\n",
        "        self.gpu_count = pynvml.nvmlDeviceGetCount()\n",
        "        for i in range(self.gpu_count):\n",
        "            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
        "            self.gpu_handles.append(handle)\n",
        "            gpu_uuid = pynvml.nvmlDeviceGetUUID(handle)\n",
        "            gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
        "            if isinstance(gpu_uuid, bytes): gpu_uuid = gpu_uuid.decode('utf-8')\n",
        "            if isinstance(gpu_name, bytes): gpu_name = gpu_name.decode('utf-8')\n",
        "            self.gpu_info.append({'index': i, 'uuid': gpu_uuid, 'name': gpu_name})\n",
        "        self.initialized = True\n",
        "\n",
        "    def collect(self) -> List[GPUMetrics]:\n",
        "        metrics = []\n",
        "        timestamp = datetime.now(timezone.utc).isoformat()\n",
        "        current_time = time.time()\n",
        "        for i, handle in enumerate(self.gpu_handles):\n",
        "            power_draw = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0\n",
        "            power_limit = pynvml.nvmlDeviceGetPowerManagementLimit(handle) / 1000.0\n",
        "            energy_delta = None\n",
        "            if i in self.last_sample_time:\n",
        "                dt = current_time - self.last_sample_time[i]\n",
        "                avg_power = (power_draw + self.last_power_draw[i]) / 2.0\n",
        "                energy_delta = avg_power * dt\n",
        "            self.last_sample_time[i] = current_time\n",
        "            self.last_power_draw[i] = power_draw\n",
        "            try:\n",
        "                util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
        "                util_gpu, util_mem = util.gpu, util.memory\n",
        "            except: util_gpu, util_mem = 0, 0\n",
        "            try: temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
        "            except: temp = 0\n",
        "            try: fan = pynvml.nvmlDeviceGetFanSpeed(handle)\n",
        "            except: fan = 0\n",
        "            try:\n",
        "                mem = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "                mem_used, mem_total = mem.used/1024/1024, mem.total/1024/1024\n",
        "            except: mem_used, mem_total = 0, 0\n",
        "\n",
        "            metrics.append(GPUMetrics(\n",
        "                timestamp=timestamp, gpu_index=i,\n",
        "                gpu_uuid=self.gpu_info[i]['uuid'],\n",
        "                gpu_name=self.gpu_info[i]['name'],\n",
        "                power_draw_w=power_draw, power_limit_w=power_limit,\n",
        "                energy_delta_j=energy_delta,\n",
        "                utilization_gpu_pct=util_gpu, utilization_memory_pct=util_mem,\n",
        "                temperature_c=temp, fan_speed_pct=fan,\n",
        "                memory_used_mb=mem_used, memory_total_mb=mem_total,\n",
        "            ))\n",
        "        return metrics\n",
        "\n",
        "    def get_gpu_count(self): return self.gpu_count\n",
        "    def get_gpu_info(self): return self.gpu_info\n",
        "\n",
        "    def shutdown(self):\n",
        "        if self.initialized:\n",
        "            pynvml.nvmlShutdown()\n",
        "            self.initialized = False\n",
        "\n",
        "    def __enter__(self): return self\n",
        "    def __exit__(self, *args): self.shutdown()\n",
        "\n",
        "print(\"GPUCollector class defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 5: Test Collector - Metrics Collection & Energy Calculation\n",
        "passed = 0\n",
        "failed = 0\n",
        "\n",
        "# --- Test 2.1: Initialization ---\n",
        "try:\n",
        "    collector = GPUCollector()\n",
        "    assert collector.initialized\n",
        "    assert collector.get_gpu_count() >= 1\n",
        "    print(f\"✅ Test 2.1: Collector initialized with {collector.get_gpu_count()} GPU(s)\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 2.1: Collector init failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 2.2: First sample (no energy delta) ---\n",
        "try:\n",
        "    metrics1 = collector.collect()\n",
        "    assert len(metrics1) == collector.get_gpu_count()\n",
        "    m = metrics1[0]\n",
        "    assert m.energy_delta_j is None, \"First sample should have no energy delta\"\n",
        "    assert m.power_draw_w >= 0\n",
        "    assert 0 <= m.utilization_gpu_pct <= 100\n",
        "    assert 0 <= m.temperature_c <= 120\n",
        "    assert m.memory_total_mb > 0\n",
        "    print(f\"✅ Test 2.2: First sample - {m.power_draw_w:.1f}W, {m.utilization_gpu_pct}% util, {m.temperature_c}°C\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 2.2: First sample failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 2.3: Second sample (with energy delta) ---\n",
        "try:\n",
        "    time.sleep(SAMPLE_INTERVAL)\n",
        "    metrics2 = collector.collect()\n",
        "    m = metrics2[0]\n",
        "    assert m.energy_delta_j is not None, \"Second sample must have energy delta\"\n",
        "    assert m.energy_delta_j > 0, \"Energy delta must be positive (GPU is drawing power)\"\n",
        "    # Sanity: E = P * t, for 2s at max 400W = 800J\n",
        "    assert m.energy_delta_j < 3000, f\"Energy delta too high: {m.energy_delta_j}J\"\n",
        "    print(f\"✅ Test 2.3: Energy delta - {m.energy_delta_j:.2f}J ({m.power_draw_w:.1f}W * ~{SAMPLE_INTERVAL}s)\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 2.3: Energy delta failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 2.4: Serialization ---\n",
        "try:\n",
        "    d = metrics2[0].to_dict()\n",
        "    required_fields = ['timestamp', 'gpu_index', 'gpu_uuid', 'gpu_name',\n",
        "                       'power_draw_w', 'power_limit_w', 'energy_delta_j',\n",
        "                       'utilization_gpu_pct', 'utilization_memory_pct',\n",
        "                       'temperature_c', 'memory_used_mb', 'memory_total_mb']\n",
        "    for field in required_fields:\n",
        "        assert field in d, f\"Missing field: {field}\"\n",
        "    print(f\"✅ Test 2.4: Serialization - all {len(required_fields)} required fields present\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 2.4: Serialization failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 2.5: GPU Info ---\n",
        "try:\n",
        "    info = collector.get_gpu_info()\n",
        "    assert len(info) >= 1\n",
        "    assert 'uuid' in info[0]\n",
        "    assert 'name' in info[0]\n",
        "    print(f\"✅ Test 2.5: GPU info - {info[0]['name']}\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 2.5: GPU info failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "collector.shutdown()\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Test 2 Results: {passed} passed, {failed} failed\")\n",
        "print(f\"{'='*50}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 3: API Endpoint Validation\n",
        "\n",
        "Test the API authentication, validation, and rate limiting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 6: Test API Endpoint\n",
        "import requests\n",
        "import json\n",
        "\n",
        "passed = 0\n",
        "failed = 0\n",
        "\n",
        "# --- Test 3.1: Health check ---\n",
        "try:\n",
        "    r = requests.get(API_ENDPOINT, timeout=10)\n",
        "    assert r.status_code == 200\n",
        "    data = r.json()\n",
        "    assert data['status'] == 'ok'\n",
        "    print(f\"✅ Test 3.1: Health check passed - {data}\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 3.1: Health check failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 3.2: Missing API key ---\n",
        "try:\n",
        "    r = requests.post(API_ENDPOINT, json=[{\"test\": True}], timeout=10)\n",
        "    assert r.status_code == 401, f\"Expected 401, got {r.status_code}\"\n",
        "    assert 'Missing API key' in r.json()['error']\n",
        "    print(f\"✅ Test 3.2: Missing key returns 401\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 3.2: Missing key test failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 3.3: Invalid API key format ---\n",
        "try:\n",
        "    r = requests.post(API_ENDPOINT,\n",
        "                      json=[{\"test\": True}],\n",
        "                      headers={'X-API-Key': 'bad_key_format'},\n",
        "                      timeout=10)\n",
        "    assert r.status_code == 401, f\"Expected 401, got {r.status_code}\"\n",
        "    assert 'format' in r.json()['error'].lower()\n",
        "    print(f\"✅ Test 3.3: Invalid key format returns 401\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 3.3: Invalid format test failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 3.4: Wrong API key (valid format) ---\n",
        "try:\n",
        "    fake_key = 'alum_' + 'A' * 59\n",
        "    r = requests.post(API_ENDPOINT,\n",
        "                      json=[{\"test\": True}],\n",
        "                      headers={'X-API-Key': fake_key},\n",
        "                      timeout=10)\n",
        "    assert r.status_code == 401, f\"Expected 401, got {r.status_code}\"\n",
        "    print(f\"✅ Test 3.4: Wrong key returns 401\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 3.4: Wrong key test failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 3.5: Empty payload (with valid key) ---\n",
        "if API_KEY:\n",
        "    try:\n",
        "        r = requests.post(API_ENDPOINT,\n",
        "                          json=[],\n",
        "                          headers={'X-API-Key': API_KEY},\n",
        "                          timeout=10)\n",
        "        assert r.status_code == 400, f\"Expected 400, got {r.status_code}\"\n",
        "        print(f\"✅ Test 3.5: Empty payload returns 400\")\n",
        "        passed += 1\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Test 3.5: Empty payload test failed - {e}\")\n",
        "        failed += 1\n",
        "\n",
        "    # --- Test 3.6: Missing required fields ---\n",
        "    try:\n",
        "        r = requests.post(API_ENDPOINT,\n",
        "                          json=[{'gpu_index': 0}],  # missing most fields\n",
        "                          headers={'X-API-Key': API_KEY},\n",
        "                          timeout=10)\n",
        "        assert r.status_code == 400, f\"Expected 400, got {r.status_code}\"\n",
        "        assert 'missing' in r.json()['error'].lower()\n",
        "        print(f\"✅ Test 3.6: Missing fields returns 400 - {r.json()['error']}\")\n",
        "        passed += 1\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Test 3.6: Missing fields test failed - {e}\")\n",
        "        failed += 1\n",
        "\n",
        "    # --- Test 3.7: Invalid value ranges ---\n",
        "    try:\n",
        "        bad_metric = {\n",
        "            'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "            'gpu_index': 0,\n",
        "            'gpu_uuid': 'test-uuid',\n",
        "            'gpu_name': 'Test GPU',\n",
        "            'power_draw_w': -100,  # Invalid: negative\n",
        "            'utilization_gpu_pct': 50,\n",
        "            'utilization_memory_pct': 50,\n",
        "            'temperature_c': 40,\n",
        "            'memory_used_mb': 1000,\n",
        "        }\n",
        "        r = requests.post(API_ENDPOINT,\n",
        "                          json=[bad_metric],\n",
        "                          headers={'X-API-Key': API_KEY},\n",
        "                          timeout=10)\n",
        "        assert r.status_code == 400, f\"Expected 400 for negative power, got {r.status_code}\"\n",
        "        print(f\"✅ Test 3.7: Negative power rejected - {r.json()['error']}\")\n",
        "        passed += 1\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Test 3.7: Value range test failed - {e}\")\n",
        "        failed += 1\n",
        "\n",
        "    # --- Test 3.8: Utilization out of range ---\n",
        "    try:\n",
        "        bad_metric = {\n",
        "            'timestamp': datetime.now(timezone.utc).isoformat(),\n",
        "            'gpu_index': 0,\n",
        "            'gpu_uuid': 'test-uuid',\n",
        "            'gpu_name': 'Test GPU',\n",
        "            'power_draw_w': 100,\n",
        "            'utilization_gpu_pct': 150,  # Invalid: > 100\n",
        "            'utilization_memory_pct': 50,\n",
        "            'temperature_c': 40,\n",
        "            'memory_used_mb': 1000,\n",
        "        }\n",
        "        r = requests.post(API_ENDPOINT,\n",
        "                          json=[bad_metric],\n",
        "                          headers={'X-API-Key': API_KEY},\n",
        "                          timeout=10)\n",
        "        assert r.status_code == 400, f\"Expected 400 for util>100, got {r.status_code}\"\n",
        "        print(f\"✅ Test 3.8: Utilization >100 rejected\")\n",
        "        passed += 1\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Test 3.8: Utilization range test failed - {e}\")\n",
        "        failed += 1\n",
        "else:\n",
        "    print(\"⏭️  Tests 3.5-3.8 skipped (no API key)\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Test 3 Results: {passed} passed, {failed} failed\")\n",
        "print(f\"{'='*50}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 4: End-to-End - Collect & Upload Real Metrics\n",
        "\n",
        "Collect real A100 metrics and upload them to the AluminatAI API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 7: End-to-End Collection & Upload\n",
        "if not API_KEY:\n",
        "    print(\"⏭️  Test 4 skipped (no API key set in Cell 2)\")\n",
        "else:\n",
        "    passed = 0\n",
        "    failed = 0\n",
        "    all_metrics = []\n",
        "\n",
        "    collector = GPUCollector()\n",
        "\n",
        "    print(f\"Collecting {NUM_SAMPLES} samples from {collector.get_gpu_info()[0]['name']}...\")\n",
        "    print()\n",
        "\n",
        "    for sample_num in range(NUM_SAMPLES):\n",
        "        metrics = collector.collect()\n",
        "        for m in metrics:\n",
        "            all_metrics.append(m.to_dict())\n",
        "\n",
        "        m = metrics[0]\n",
        "        energy_str = f\"{m.energy_delta_j:.2f}J\" if m.energy_delta_j else \"N/A\"\n",
        "        print(f\"  Sample {sample_num+1}/{NUM_SAMPLES}: \"\n",
        "              f\"{m.power_draw_w:.1f}W | \"\n",
        "              f\"{m.utilization_gpu_pct}% util | \"\n",
        "              f\"{m.temperature_c}°C | \"\n",
        "              f\"E={energy_str}\")\n",
        "\n",
        "        if sample_num < NUM_SAMPLES - 1:\n",
        "            time.sleep(SAMPLE_INTERVAL)\n",
        "\n",
        "    collector.shutdown()\n",
        "\n",
        "    print(f\"\\nCollected {len(all_metrics)} metric records.\")\n",
        "\n",
        "    # --- Test 4.1: Upload batch ---\n",
        "    try:\n",
        "        session = requests.Session()\n",
        "        session.headers.update({\n",
        "            'Content-Type': 'application/json',\n",
        "            'X-API-Key': API_KEY,\n",
        "        })\n",
        "\n",
        "        r = session.post(API_ENDPOINT, json=all_metrics, timeout=30)\n",
        "        assert r.status_code == 200, f\"Upload failed: {r.status_code} - {r.text}\"\n",
        "        data = r.json()\n",
        "        assert data['success'] == True\n",
        "        assert data['inserted'] == len(all_metrics)\n",
        "        print(f\"\\n✅ Test 4.1: Uploaded {data['inserted']} metrics successfully\")\n",
        "\n",
        "        # Check rate limit headers\n",
        "        rl_limit = r.headers.get('X-RateLimit-Limit')\n",
        "        rl_remaining = r.headers.get('X-RateLimit-Remaining')\n",
        "        if rl_limit:\n",
        "            print(f\"   Rate limit: {rl_remaining}/{rl_limit} remaining\")\n",
        "        passed += 1\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Test 4.1: Upload failed - {e}\")\n",
        "        failed += 1\n",
        "\n",
        "    # --- Test 4.2: Verify data integrity ---\n",
        "    try:\n",
        "        total_energy_j = sum(m['energy_delta_j'] for m in all_metrics\n",
        "                             if m['energy_delta_j'] is not None)\n",
        "        total_energy_kwh = total_energy_j / 3_600_000\n",
        "        avg_power = sum(m['power_draw_w'] for m in all_metrics) / len(all_metrics)\n",
        "        expected_energy = avg_power * SAMPLE_INTERVAL * (NUM_SAMPLES - 1)  # approximate\n",
        "\n",
        "        # Allow 50% tolerance for energy calculation\n",
        "        assert total_energy_j > 0, \"Total energy should be positive\"\n",
        "        ratio = total_energy_j / expected_energy if expected_energy > 0 else 1\n",
        "        assert 0.3 < ratio < 3.0, f\"Energy calculation seems off: ratio={ratio:.2f}\"\n",
        "\n",
        "        print(f\"✅ Test 4.2: Data integrity check\")\n",
        "        print(f\"   Total energy: {total_energy_j:.2f}J ({total_energy_kwh:.6f} kWh)\")\n",
        "        print(f\"   Avg power: {avg_power:.1f}W\")\n",
        "        print(f\"   Expected ~{expected_energy:.1f}J, got {total_energy_j:.1f}J (ratio: {ratio:.2f})\")\n",
        "        passed += 1\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Test 4.2: Data integrity check failed - {e}\")\n",
        "        failed += 1\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Test 4 Results: {passed} passed, {failed} failed\")\n",
        "    print(f\"{'='*50}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 5: Stress Test Under GPU Load\n",
        "\n",
        "Create real GPU load with PyTorch, then collect metrics to verify the agent captures actual workload."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 8: Stress Test - GPU Load + Metrics Collection\n",
        "import torch\n",
        "import threading\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"❌ CUDA not available - skipping stress test\")\n",
        "else:\n",
        "    passed = 0\n",
        "    failed = 0\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Memory: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB\")\n",
        "\n",
        "    # --- Collect idle baseline ---\n",
        "    collector = GPUCollector()\n",
        "    _ = collector.collect()  # prime the energy calculator\n",
        "    time.sleep(1)\n",
        "    idle_metrics = collector.collect()[0]\n",
        "    idle_power = idle_metrics.power_draw_w\n",
        "    idle_util = idle_metrics.utilization_gpu_pct\n",
        "    print(f\"\\nIdle baseline: {idle_power:.1f}W, {idle_util}% util\")\n",
        "\n",
        "    # --- Create GPU load ---\n",
        "    print(\"\\nStarting GPU load (large matrix multiply)...\")\n",
        "    stop_event = threading.Event()\n",
        "\n",
        "    def gpu_stress():\n",
        "        a = torch.randn(8192, 8192, device=device)\n",
        "        b = torch.randn(8192, 8192, device=device)\n",
        "        while not stop_event.is_set():\n",
        "            c = torch.matmul(a, b)\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "    stress_thread = threading.Thread(target=gpu_stress)\n",
        "    stress_thread.start()\n",
        "\n",
        "    # Wait for GPU to ramp up\n",
        "    time.sleep(3)\n",
        "\n",
        "    # --- Collect under load ---\n",
        "    load_samples = []\n",
        "    for i in range(5):\n",
        "        metrics = collector.collect()\n",
        "        m = metrics[0]\n",
        "        load_samples.append(m)\n",
        "        print(f\"  Load sample {i+1}: {m.power_draw_w:.1f}W, \"\n",
        "              f\"{m.utilization_gpu_pct}% util, \"\n",
        "              f\"{m.temperature_c}°C, \"\n",
        "              f\"E={m.energy_delta_j:.2f}J\" if m.energy_delta_j else \"N/A\")\n",
        "        time.sleep(SAMPLE_INTERVAL)\n",
        "\n",
        "    # Stop GPU load\n",
        "    stop_event.set()\n",
        "    stress_thread.join()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # --- Test 5.1: Power increased under load ---\n",
        "    try:\n",
        "        avg_load_power = sum(s.power_draw_w for s in load_samples) / len(load_samples)\n",
        "        assert avg_load_power > idle_power * 1.1, (\n",
        "            f\"Power should increase under load: idle={idle_power:.1f}W, load={avg_load_power:.1f}W\")\n",
        "        print(f\"\\n✅ Test 5.1: Power increased - idle: {idle_power:.1f}W -> load: {avg_load_power:.1f}W\")\n",
        "        passed += 1\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Test 5.1: Power increase test failed - {e}\")\n",
        "        failed += 1\n",
        "\n",
        "    # --- Test 5.2: Utilization detected ---\n",
        "    try:\n",
        "        avg_util = sum(s.utilization_gpu_pct for s in load_samples) / len(load_samples)\n",
        "        assert avg_util > 50, f\"Expected >50% utilization under load, got {avg_util:.1f}%\"\n",
        "        print(f\"✅ Test 5.2: Utilization under load - {avg_util:.1f}%\")\n",
        "        passed += 1\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Test 5.2: Utilization test failed - {e}\")\n",
        "        failed += 1\n",
        "\n",
        "    # --- Test 5.3: Energy calculation accurate ---\n",
        "    try:\n",
        "        total_energy = sum(s.energy_delta_j for s in load_samples if s.energy_delta_j)\n",
        "        assert total_energy > 0, \"Energy should be positive\"\n",
        "        # Rough check: total energy should be close to avg_power * total_time\n",
        "        total_time = SAMPLE_INTERVAL * len(load_samples)\n",
        "        expected = avg_load_power * total_time\n",
        "        ratio = total_energy / expected\n",
        "        assert 0.3 < ratio < 3.0, f\"Energy/expected ratio off: {ratio:.2f}\"\n",
        "        print(f\"✅ Test 5.3: Energy under load - {total_energy:.1f}J (expected ~{expected:.1f}J)\")\n",
        "        passed += 1\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Test 5.3: Energy calculation test failed - {e}\")\n",
        "        failed += 1\n",
        "\n",
        "    # --- Test 5.4: Upload load metrics ---\n",
        "    if API_KEY:\n",
        "        try:\n",
        "            load_dicts = [s.to_dict() for s in load_samples]\n",
        "            r = requests.post(API_ENDPOINT,\n",
        "                              json=load_dicts,\n",
        "                              headers={'X-API-Key': API_KEY},\n",
        "                              timeout=30)\n",
        "            assert r.status_code == 200, f\"Upload failed: {r.status_code}\"\n",
        "            print(f\"✅ Test 5.4: Uploaded {len(load_dicts)} stress test metrics\")\n",
        "            passed += 1\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Test 5.4: Upload stress metrics failed - {e}\")\n",
        "            failed += 1\n",
        "    else:\n",
        "        print(\"⏭️  Test 5.4 skipped (no API key)\")\n",
        "\n",
        "    collector.shutdown()\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Test 5 Results: {passed} passed, {failed} failed\")\n",
        "    print(f\"{'='*50}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 6: API Key Uniqueness & Security\n",
        "\n",
        "Verify API key format and generation security."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 9: API Key Format & Security Tests\n",
        "import secrets\n",
        "import string\n",
        "import re\n",
        "\n",
        "passed = 0\n",
        "failed = 0\n",
        "\n",
        "CHARSET = 'ABCDEFGHJKLMNPQRSTUVWXYZabcdefghjkmnpqrstuvwxyz23456789'\n",
        "\n",
        "def generate_api_key():\n",
        "    \"\"\"Mirror the server-side generation logic\"\"\"\n",
        "    key = 'alum_'\n",
        "    for _ in range(59):\n",
        "        key += secrets.choice(CHARSET)\n",
        "    return key\n",
        "\n",
        "# --- Test 6.1: Key format ---\n",
        "try:\n",
        "    key = generate_api_key()\n",
        "    assert key.startswith('alum_'), \"Key must start with 'alum_'\"\n",
        "    assert len(key) == 64, f\"Key must be 64 chars, got {len(key)}\"\n",
        "    # Check all chars are from valid charset\n",
        "    for c in key[5:]:\n",
        "        assert c in CHARSET, f\"Invalid char in key: {c}\"\n",
        "    print(f\"✅ Test 6.1: Key format valid - alum_{'*'*10}...{key[-4:]} (len={len(key)})\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 6.1: Key format test failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 6.2: Key uniqueness ---\n",
        "try:\n",
        "    keys = set()\n",
        "    num_keys = 10000\n",
        "    for _ in range(num_keys):\n",
        "        keys.add(generate_api_key())\n",
        "    assert len(keys) == num_keys, f\"Collision detected: {num_keys - len(keys)} duplicates\"\n",
        "    print(f\"✅ Test 6.2: Generated {num_keys} unique keys (0 collisions)\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 6.2: Uniqueness test failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 6.3: Entropy check ---\n",
        "try:\n",
        "    import math\n",
        "    charset_size = len(CHARSET)  # 54 chars\n",
        "    key_length = 59  # random portion\n",
        "    entropy_bits = math.log2(charset_size) * key_length\n",
        "    assert entropy_bits > 256, f\"Entropy too low: {entropy_bits:.1f} bits\"\n",
        "    print(f\"✅ Test 6.3: Entropy - {entropy_bits:.1f} bits ({charset_size}^{key_length}) - exceeds 256-bit threshold\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 6.3: Entropy test failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 6.4: No ambiguous characters ---\n",
        "try:\n",
        "    ambiguous = set('0OIl1')\n",
        "    charset_set = set(CHARSET)\n",
        "    overlap = ambiguous & charset_set\n",
        "    assert len(overlap) == 0, f\"Ambiguous chars found in charset: {overlap}\"\n",
        "    print(f\"✅ Test 6.4: No ambiguous characters (0, O, I, l, 1 excluded)\")\n",
        "    passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test 6.4: Ambiguous chars test failed - {e}\")\n",
        "    failed += 1\n",
        "\n",
        "# --- Test 6.5: Validate current API key format (if provided) ---\n",
        "if API_KEY:\n",
        "    try:\n",
        "        assert API_KEY.startswith('alum_'), \"Your API key doesn't start with 'alum_'\"\n",
        "        assert len(API_KEY) == 64, f\"Your API key should be 64 chars, got {len(API_KEY)}\"\n",
        "        print(f\"✅ Test 6.5: Your API key format is valid\")\n",
        "        passed += 1\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Test 6.5: Your API key format invalid - {e}\")\n",
        "        failed += 1\n",
        "else:\n",
        "    print(\"⏭️  Test 6.5 skipped (no API key)\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Test 6 Results: {passed} passed, {failed} failed\")\n",
        "print(f\"{'='*50}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test 7: Continuous Monitoring Demo\n",
        "\n",
        "Run a short continuous monitoring session (like the real agent would) and display a summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 10: Continuous Monitoring - 60 Second Demo Run\n",
        "DEMO_DURATION = 60  # seconds\n",
        "DEMO_INTERVAL = 5   # seconds\n",
        "\n",
        "print(f\"Running continuous monitoring for {DEMO_DURATION}s (interval: {DEMO_INTERVAL}s)\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"{'#':>4} | {'Time':>8} | {'Power':>8} | {'Util':>5} | {'Temp':>5} | {'Mem MB':>8} | {'Energy J':>10}\")\n",
        "print(f\"{'-'*4}-+-{'-'*8}-+-{'-'*8}-+-{'-'*5}-+-{'-'*5}-+-{'-'*8}-+-{'-'*10}\")\n",
        "\n",
        "collector = GPUCollector()\n",
        "all_samples = []\n",
        "total_energy = 0.0\n",
        "start_time = time.time()\n",
        "sample_num = 0\n",
        "\n",
        "while time.time() - start_time < DEMO_DURATION:\n",
        "    loop_start = time.time()\n",
        "    metrics = collector.collect()\n",
        "    sample_num += 1\n",
        "    m = metrics[0]\n",
        "    all_samples.append(m)\n",
        "\n",
        "    if m.energy_delta_j:\n",
        "        total_energy += m.energy_delta_j\n",
        "\n",
        "    energy_str = f\"{m.energy_delta_j:.2f}\" if m.energy_delta_j else \"N/A\"\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"{sample_num:>4} | {elapsed:>7.1f}s | {m.power_draw_w:>7.1f}W | {m.utilization_gpu_pct:>4}% | {m.temperature_c:>4}C | {m.memory_used_mb:>7.0f} | {energy_str:>10}\")\n",
        "\n",
        "    sleep_time = max(0, DEMO_INTERVAL - (time.time() - loop_start))\n",
        "    if sleep_time > 0 and time.time() - start_time + sleep_time < DEMO_DURATION:\n",
        "        time.sleep(sleep_time)\n",
        "\n",
        "collector.shutdown()\n",
        "\n",
        "# Summary\n",
        "runtime = time.time() - start_time\n",
        "total_kwh = total_energy / 3_600_000\n",
        "avg_power = sum(s.power_draw_w for s in all_samples) / len(all_samples)\n",
        "max_power = max(s.power_draw_w for s in all_samples)\n",
        "avg_temp = sum(s.temperature_c for s in all_samples) / len(all_samples)\n",
        "max_temp = max(s.temperature_c for s in all_samples)\n",
        "avg_util = sum(s.utilization_gpu_pct for s in all_samples) / len(all_samples)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"MONITORING SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Runtime:         {runtime:.1f}s\")\n",
        "print(f\"Samples:         {len(all_samples)}\")\n",
        "print(f\"GPU:             {all_samples[0].gpu_name}\")\n",
        "print(f\"Avg Power:       {avg_power:.1f}W (max: {max_power:.1f}W)\")\n",
        "print(f\"Avg Utilization: {avg_util:.1f}%\")\n",
        "print(f\"Avg Temperature: {avg_temp:.1f}°C (max: {max_temp}°C)\")\n",
        "print(f\"Total Energy:    {total_energy:.2f}J ({total_kwh:.6f} kWh)\")\n",
        "print(f\"Est. Cost:       ${total_kwh * 0.12:.6f} (@ $0.12/kWh)\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Upload if API key available\n",
        "if API_KEY:\n",
        "    metrics_dicts = [s.to_dict() for s in all_samples]\n",
        "    r = requests.post(API_ENDPOINT,\n",
        "                      json=metrics_dicts,\n",
        "                      headers={'X-API-Key': API_KEY},\n",
        "                      timeout=30)\n",
        "    if r.status_code == 200:\n",
        "        print(f\"\\n✅ Uploaded {len(metrics_dicts)} samples to AluminatAI dashboard\")\n",
        "    else:\n",
        "        print(f\"\\n❌ Upload failed: {r.status_code} - {r.text}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "All tests completed. Check your [AluminatAI Dashboard](https://aluminatiai-landing.vercel.app/dashboard) to see the uploaded metrics.\n",
        "\n",
        "### What was tested:\n",
        "- **Test 1**: Raw NVML access (power, util, temp, memory)\n",
        "- **Test 2**: GPUCollector class + energy delta calculation\n",
        "- **Test 3**: API auth validation (missing key, bad format, wrong key, bad payloads)\n",
        "- **Test 4**: End-to-end collect + upload + data integrity\n",
        "- **Test 5**: Stress test under real GPU load (matrix multiply)\n",
        "- **Test 6**: API key format, uniqueness, and entropy\n",
        "- **Test 7**: Continuous 60-second monitoring demo"
      ]
    }
  ]
}
